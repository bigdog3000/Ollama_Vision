# ğŸ–¼ï¸ OllamaÂ VisionÂ Analyzer

Â Â 

A oneâ€‘file Gradio web app for rapid visual scene understanding with **llama3.2â€‘vision** served through **[Ollama](https://ollama.ai/)**. Select or upload a picture, hit **Analyze**, and receive a rich, structured JSON description including detected objects, scene, colors, text and more.

---

## âœ¨Â Features

* **Local & Upload Sources**Â â€“ choose from `~/images` or dragâ€‘andâ€‘drop a file.
* **Live Preview**Â â€“ instant thumbnail before analysis.
* **Custom Prompts**Â â€“ override the default analysis request with your own instructions.
* **Schemaâ€‘Constrained Output**Â â€“ responses are validated JSON that fits the `ImageDescription`Â schema.
* **Model Health Check**Â â€“ oneâ€‘click readiness probe for the Ollama vision model.
* **Selfâ€‘contained**Â â€“ single Python script, no external frontâ€‘end build.
* **Shareable**Â â€“Â `demo.launch(share=True)` makes a public link via Gradio.

---

## ğŸš€Â QuickÂ start

```bash
# 1.Â Clone the repo (or copy the single script)
git clone https://github.com/bigdog3000/Ollama_Vision.git
cd Ollama_Vision

# 2.Â Create a virtual env (recommended)
conda create -n ollamaimage python=3.11 -y
conda activate ollamaimage

# 3.Â Install deps
pip install -r requirements.txt
#   or directly:
# pip install gradio pillow pydantic ollama

# 4.Â Run Ollama with the llama3.2â€‘vision model
ollama serve &               # starts the Ollama backend
ollama pull llama3.2-vision  # firstâ€‘time model download

# 5.Â Launch the Gradio app
gadio_vision_ollama.py  # listens on http://localhost:8040
```

> **Note**: By default the app scans `~/images` for selectable pictures. Symlink or copy whatever you like into that folder, or edit `list_image_files()` to point elsewhere.

---

## ğŸ§©Â JSONÂ Schema

```text
ImageDescription
 â”œâ”€ summary:          str
 â”œâ”€ objects[]
 â”‚   â”œâ”€ name:         str
 â”‚   â”œâ”€ confidence:   float  (0â€“1)
 â”‚   â””â”€ attributes:   str
 â”œâ”€ scene:            str
 â”œâ”€ colors[]:         str
 â”œâ”€ time_of_day:      "Morning" | "Afternoon" | "Evening" | "Night"
 â”œâ”€ setting:          "Indoor" | "Outdoor" | "Unknown"
 â””â”€ text_content:     str | null
```

All model responses are expected to match this shape, making them easy to postâ€‘process or feed into downstream pipelines.

---

## ğŸ–¥ï¸Â Screenshots


![image](https://github.com/user-attachments/assets/271af9bb-bafe-43fc-9d63-b7e599b54625)


---

## ğŸ”§Â Configuration

| Variable      | Default    | Description                                     |
| ------------- | ---------- | ----------------------------------------------- |
| `IMAGES_DIR`  | `~/images` | Folder scanned for the dropdown list            |
| `SERVER_PORT` | `8040`     | Change `demo.launch()` call to override port    |
| `SHARE`       | `True`     | SetÂ `share=False` to disable public Gradio link |

You can also point **Ollama** at a remote server via the `OLLAMA_HOST` environment variable.

---

## ğŸ› ï¸Â Extending

* **Alternative models**: swap `llama3.2-vision` in both `check_model_ready()` and `analyze_image()`.
* **Postâ€‘processing**: parse `output_box` JSON into domainâ€‘specific dashboards or databases.
* **Batch mode**: wrap `analyze_image()` in a loop to process many files headlessly.

---

## ğŸ™Â Acknowledgements

* [Gradio](https://github.com/gradio-app/gradio) â€“ deadâ€‘simple ML frontâ€‘ends.
* [Ollama](https://github.com/jmorganca/ollama) â€“ local model runner with vision support.
* LlamaÂ 3Â contributors for the underlying vision model.

---

## ğŸ“œÂ License

Released under the [MIT License](LICENSE).

> Made with â¤ï¸Â and llamas.
